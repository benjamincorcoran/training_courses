{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15747467-aaee-4881-94e1-8c21b130f127",
   "metadata": {},
   "source": [
    "# Introduction to Data Linking\n",
    "\n",
    "## Contents \n",
    "* [String Comparison](#string-comparison)\n",
    "* [Deterministic Matching](#deterministic-matching)\n",
    "* [M and U Values](#m-and-u-values)\n",
    "* [Expectation Maximisation](#expectation-maximisation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f4a7c-e21a-4acf-8fcb-caecca01abb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## String Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d53c991-0892-495a-b86e-7744b0ab939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: metaphone in /opt/miniconda3/lib/python3.9/site-packages (0.6)\n",
      "Requirement already satisfied: jellyfish in /opt/miniconda3/lib/python3.9/site-packages (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "# Required libraries \n",
    "!pip install metaphone jellyfish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7318e2fe-18bb-480e-b352-87121b5f5547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "String distance metrics for niamh and nieve: \n",
      "\n",
      "\n",
      "  Levenshtein edit distance: 3\n",
      "\n",
      "  standardised levenshtein score: 0.4\n",
      "\n",
      "  bigram score: 0.333\n",
      "\n",
      "  Jaro similarity score: 0.6\n",
      "\n",
      "  jaro-winkler similarity score: 0.6\n",
      "\n",
      "\n",
      "Phonetic encodings: \n",
      "\n",
      "\n",
      "  soundex encoding of string 1: N500\n",
      "\n",
      "  soundex encoding of string 2: N100\n",
      "\n",
      "  double metaphone encoding of string 1: ('NM', '')\n",
      "\n",
      "  double metaphone encoding of string 2: ('NF', '')\n"
     ]
    }
   ],
   "source": [
    "# load required packages\n",
    "import jellyfish\n",
    "from metaphone import doublemetaphone\n",
    "import logging\n",
    "\n",
    "# strings to alter\n",
    "string1 = \"niamh\"\n",
    "string2 = \"nieve\"\n",
    "\n",
    "# soundex phonetic encoding\n",
    "soundex1 = jellyfish.soundex(string1)\n",
    "soundex2 = jellyfish.soundex(string2)\n",
    "\n",
    "metaphone1 = doublemetaphone(string1)\n",
    "metaphone2 = doublemetaphone(string2)\n",
    "\n",
    "# levenshtein edit distance\n",
    "lev_distance = jellyfish.levenshtein_distance(string1, string2)\n",
    "\n",
    "# standardised levenshtein edit distance\n",
    "standardised_lev = round((1 - ((jellyfish.levenshtein_distance(string1, string2))/max(len(string1), len(string2)))),3)\n",
    "\n",
    "# jaro similarity score\n",
    "jaro = round(jellyfish.jaro_distance(string1, string2),3)\n",
    "\n",
    "# jaro-winkler similarity score\n",
    "jaro_winkler = round(jellyfish.jaro_winkler_similarity(string1, string2),3)\n",
    "\n",
    "# bigram string comparison (brace yourself for lots of code)\n",
    "QGRAM_END_CHAR =   chr(2)\n",
    "QGRAM_START_CHAR = chr(1)\n",
    "\n",
    "def qgram(str1, str2, q=2, common_divisor = 'average', min_threshold = None,\n",
    "          padded=True):\n",
    "    \"\"\"Return approximate string comparator measure (between 0.0 and 1.0)\n",
    "       using q-grams (with default bigrams: q = 2).\n",
    "       USAGE:\n",
    "           score = qgram(str1, str2, q, common_divisor, min_threshold, padded)\n",
    "       ARGUMENTS:\n",
    "           str1            The first string\n",
    "           str2            The second string\n",
    "           q               The length of the q-grams to be used. Must be at least 1.\n",
    "           common_divisor  Method of how to calculate the divisor, it can be set to\n",
    "                           'average','shortest', or 'longest' , and is calculated\n",
    "                           according to the lengths of the two input strings\n",
    "           min_threshold   Minimum threshold between 0 and 1\n",
    "           padded          If set to True (default), the beginnng and end of the\n",
    "                           strings will be padded with (q-1) special characters, if\n",
    "                           False no padding will be done.\n",
    "       DESCRIPTION:\n",
    "           q-grams are q-character sub-strings contained in a string. For example,\n",
    "           'peter' contains the bigrams (q=2): ['pe','et','te','er'].\n",
    "           Padding will result in specific q-grams at the beginning and end of a\n",
    "           string, for example 'peter' converted into padded bigrams (q=2) will result\n",
    "           in the following 2-gram list: ['*p','pe','et','te','er','r@'], with '*'\n",
    "           illustrating the start and '@' the end character.\n",
    "           This routine counts the number of common q-grams and divides by the\n",
    "           average number of q-grams. The resulting number is returned.\n",
    "    \"\"\"\n",
    "\n",
    "    if (q < 1):\n",
    "        logging.exception('Illegal value for q: %d (must be at least 1)' % (q))\n",
    "        raise Exception\n",
    "\n",
    "    # Quick check if the strings are empty or the same - - - - - - - - - - - - -\n",
    "    #\n",
    "    if (str1 == '') or (str2 == ''):\n",
    "        return 0.0\n",
    "    elif (str1 == str2):\n",
    "        return 1.0\n",
    "\n",
    "    # Calculate number of q-grams in strings (plus start and end characters) - -\n",
    "    #\n",
    "    if (padded == True):\n",
    "        num_qgram1 = len(str1)+q-1\n",
    "        num_qgram2 = len(str2)+q-1\n",
    "    else:\n",
    "        num_qgram1 = max(len(str1)-(q-1),0)  # Make sure its not negative\n",
    "        num_qgram2 = max(len(str2)-(q-1),0)\n",
    "\n",
    "    # Check if there are q-grams at all from both strings - - - - - - - - - - - -\n",
    "    # (no q-grams if length of a string is less than q)\n",
    "    #\n",
    "    if ((padded == False) and (min(num_qgram1, num_qgram2) == 0)):\n",
    "        return 0.0\n",
    "\n",
    "    # Calculate the divisor - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    #\n",
    "    if (common_divisor not in ['average','shortest','longest']):\n",
    "        logging.exception('Illegal value for common divisor: %s' % \\\n",
    "                          (common_divisor))\n",
    "        raise Exception\n",
    "\n",
    "    if (common_divisor == 'average'):\n",
    "        divisor = 0.5*(num_qgram1+num_qgram2)  # Compute average number of q-grams\n",
    "    elif (common_divisor == 'shortest'):\n",
    "        divisor = min(num_qgram1,num_qgram2)\n",
    "    else:  # Longest\n",
    "        divisor = max(num_qgram1,num_qgram2)\n",
    "\n",
    "    # Use number of q-grams to quickly check for minimum threshold - - - - - - -\n",
    "    #\n",
    "    if (min_threshold != None):\n",
    "        if (isinstance(min_threshold, float)) and (min_threshold > 0.0) and \\\n",
    "           (min_threshold > 0.0):\n",
    "\n",
    "            max_common_qgram = min(num_qgram1,num_qgram2)\n",
    "\n",
    "        w = float(max_common_qgram) / float(divisor)\n",
    "\n",
    "        if (w  < min_threshold):\n",
    "            return 0.0  # Similariy is smaller than minimum threshold\n",
    "\n",
    "        else:\n",
    "            logging.exception('Illegal value for minimum threshold (not between' + \\\n",
    "                              ' 0 and 1): %f' % (min_threshold))\n",
    "            raise Exception\n",
    "\n",
    "        # Add start and end characters (padding) - - - - - - - - - - - - - - - - - -\n",
    "        #\n",
    "    if (padded == True):\n",
    "        qgram_str1 = (q-1)*QGRAM_START_CHAR+str1+(q-1)*QGRAM_END_CHAR\n",
    "        qgram_str2 = (q-1)*QGRAM_START_CHAR+str2+(q-1)*QGRAM_END_CHAR\n",
    "    else:\n",
    "        qgram_str1 = str1\n",
    "        qgram_str2 = str2\n",
    "\n",
    "    # Make a list of q-grams for both strings - - - - - - - - - - - - - - - - - -\n",
    "    #\n",
    "    qgram_list1 = [qgram_str1[i:i+q] for i in range(len(qgram_str1) - (q-1))]\n",
    "    qgram_list2 = [qgram_str2[i:i+q] for i in range(len(qgram_str2) - (q-1))]\n",
    "\n",
    "    # Get common q-grams  - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    #\n",
    "    common = 0\n",
    "\n",
    "    if (num_qgram1 < num_qgram2):  # Count using the shorter q-gram list\n",
    "        short_qgram_list = qgram_list1\n",
    "        long_qgram_list =  qgram_list2\n",
    "    else:\n",
    "        short_qgram_list = qgram_list2\n",
    "        long_qgram_list =  qgram_list1\n",
    "\n",
    "    for q_gram in short_qgram_list:\n",
    "        if (q_gram in long_qgram_list):\n",
    "            common += 1\n",
    "            long_qgram_list.remove(q_gram)  # Remove the counted q-gram\n",
    "\n",
    "    w = float(common) / float(divisor)\n",
    "\n",
    "    assert (w >= 0.0) and (w <= 1.0), 'Similarity weight outside 0-1: %f' % (w)\n",
    "\n",
    "    # A log message - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\n",
    "    #\n",
    "    logging.debug('%d-gram comparator string \"%s\" with \"%s\" value: %.3f' % \\\n",
    "                  (q, str1, str2, w))\n",
    "    return round(w,3)\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "def bigram(str1, str2, min_threshold = None):\n",
    "    \"\"\"For backwards compatibility.\n",
    "    \"\"\"\n",
    "    return qgram(str1, str2, 2, 'average', min_threshold)\n",
    "\n",
    "print(\"\\nString distance metrics for \" + string1 + \" and \" + string2 +\\\n",
    "      \": \\n\\n\\n  Levenshtein edit distance: \" + str(lev_distance) +\\\n",
    "      \"\\n\\n  standardised levenshtein score: \" + str(standardised_lev) +\\\n",
    "      \"\\n\\n  bigram score: \" +str(qgram(string1, string2)) +\\\n",
    "      \"\\n\\n  Jaro similarity score: \" + str(jaro) +\\\n",
    "      \"\\n\\n  jaro-winkler similarity score: \" + str(jaro_winkler)+\\\n",
    "      \"\\n\\n\\nPhonetic encodings: \\n\\n\\n  soundex encoding of string 1: \" + str(soundex1) +\\\n",
    "      \"\\n\\n  soundex encoding of string 2: \" + str(soundex2) +\\\n",
    "      \"\\n\\n  double metaphone encoding of string 1: \" + str(metaphone1) +\n",
    "      \"\\n\\n  double metaphone encoding of string 2: \" + str(metaphone2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26dcf502-02e0-49b2-be24-a7e34d63728e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deterministic Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a4c0253b-6e27-49b7-9b57-35b7eb67d856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact matching\n",
      "True positives = 4474\n",
      "False positives = 0\n",
      "Unmatched = 2960\n",
      "\n",
      "Matchkey = firstname, surname, dob, sex, postcode\n",
      "Multiple matches = 0\n",
      "True positives = 484\n",
      "False positives = 6\n",
      "Unmatched = 2470\n",
      "\n",
      "Matchkey = first1, middle1, sur1, dob, sex, postcode\n",
      "Multiple matches = 0\n",
      "True positives = 108\n",
      "False positives = 0\n",
      "Unmatched = 2362\n",
      "\n",
      "Matchkey = first1, middle1, sur1, monthbirth, yearbirth, sex, postcode\n",
      "Multiple matches = 0\n",
      "True positives = 78\n",
      "False positives = 2\n",
      "Unmatched = 2282\n",
      "\n",
      "Matchkey = first1, middle1, sur1, daybirth, yearbirth, sex, postcode\n",
      "Multiple matches = 0\n",
      "True positives = 7\n",
      "False positives = 0\n",
      "Unmatched = 2275\n",
      "\n",
      "Matchkey = first1, middle1, sur1, monthbirth, daybirth, sex, postcode\n",
      "Multiple matches = 0\n",
      "True positives = 18\n",
      "False positives = 3\n",
      "Unmatched = 2254\n",
      "\n",
      "Matchkey = first1, sur1, dob, sex, postcode\n",
      "Multiple matches = 0\n",
      "True positives = 264\n",
      "False positives = 0\n",
      "Unmatched = 1990\n",
      "\n",
      "Matchkey = firstname, surname, dob, sex, pcarea\n",
      "Multiple matches = 2\n",
      "True positives = 148\n",
      "False positives = 17\n",
      "Unmatched = 1825\n",
      "\n",
      "Matchkey = first1, sur1, dob, sex, pcarea\n",
      "Multiple matches = 2\n",
      "True positives = 10\n",
      "False positives = 0\n",
      "Unmatched = 1815\n",
      "\n",
      "Matchkey = firstname, surname, dob, postcode\n",
      "Multiple matches = 0\n",
      "True positives = 13\n",
      "False positives = 1\n",
      "Unmatched = 1801\n",
      "\n",
      "Matchkey = shortfirst1, shortsur1, dob, sex, pcarea\n",
      "Multiple matches = 12\n",
      "True positives = 670\n",
      "False positives = 0\n",
      "Unmatched = 1131\n",
      "\n",
      "Matchkey = initials, dob, sex, pcarea\n",
      "Multiple matches = 10\n",
      "True positives = 437\n",
      "False positives = 0\n",
      "Unmatched = 694\n",
      "\n",
      "Overall match rate\n",
      "True positives = 6711\n",
      "False positives = 29\n",
      "False negatives = 723\n",
      "\n",
      "Precision = 99.57%\n",
      "Recall = 90.27%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "\n",
    "\n",
    "def read_data():\n",
    "    # Read in datasets to link\n",
    "    dfA = pd.read_csv('./raw_data/working_data_a.csv')\n",
    "    dfB = pd.read_csv('./raw_data/working_data_b.csv')\n",
    "    # Make sure column types correct\n",
    "    dfA = dfA.astype({\"id_a\":str, \"firstname_a\":str, \"middlename_a\":str, \"surname_a\":str,\n",
    "                      \"sex_a\":str, \"dob_a\":str, \"postcode_a\": str})\n",
    "    \n",
    "    dfB = dfB.astype({\"id_b\":str, \"firstname_b\":str, \"middlename_b\":str, \"surname_b\":str,\n",
    "                      \"sex_b\":str, \"dob_b\":str, \"postcode_b\": str})\n",
    "    # Drop the ident_a and ident_b columns as we don't need them\n",
    "    return (dfA.drop(['ident_a','ident_b'], axis=1), dfB.drop(['ident_a','ident_b'], axis=1))\n",
    "\n",
    "\n",
    "def clean_data(df, letter):\n",
    "    # Convert standardise the name variables\n",
    "    df = standardise_names(df, letter)\n",
    "    # Standardise the postcodes\n",
    "    df = standardise_postcode(df, 'postcode_' + letter)\n",
    "    # Standardise the sex\n",
    "    df = standardise_sex(df, 'sex_' + letter)\n",
    "    # Standardise the date of birth\n",
    "    df = standardise_dob(df, 'dob_' + letter)\n",
    "    return df\n",
    "\n",
    "\n",
    "def standardise_names(df, letter):\n",
    "    for name in add_subscript(letter, [\"firstname\", \"middlename\", \"surname\"]):\n",
    "        # Ensure all names are uppercase\n",
    "        df[name] = df[name].str.upper()\n",
    "        # Ensure names contain no whitespace at the beginning or end, only have \n",
    "        # single spaces internally and convert all hyphens to spaces\n",
    "        df[name] = df[name].str.replace('-',' ').str.replace('  ',' ').str.strip()\n",
    "        # Replace any empty names or the name \"NAN\" with 'None'\n",
    "        df[name] = np.where((df[name] == 'NAN') | (df[name] == ''), None, df[name])\n",
    "        # Convert the column type to str\n",
    "        df.astype({name:str}, copy=False)\n",
    "    # Split each name into two variables on the delimiter ' '. Later name variables\n",
    "    # will be 'None' if there are not two names in that column. Titles will be \n",
    "    # stripped off into a title column\n",
    "    return split_names(df, letter)\n",
    "\n",
    "\n",
    "def split_names(df, letter):  \n",
    "    # Split firstname. As it may contain a title and two names, we need to temporarily\n",
    "    # create three firstname variables (the last of which will be dropped later)\n",
    "    f1, f2, f3 = add_subscript(letter, ['first1', 'first2','first3'])\n",
    "    df[[f1, f2, f3]] = df['firstname_'+letter].str.split(' ', n=2, expand=True)\n",
    "    # Remove any titles by putting them in a separate title column\n",
    "    titles = ['MR', 'MRS', 'MISS', 'MS', 'DR']\n",
    "    df['title_'+letter] = np.where(df[f1].isin(titles), df[f1], None)\n",
    "    # If firstname1 is a title, swap the order of the forename variables so it\n",
    "    # becomes forename3 (the column we will drop)\n",
    "    df[f1], df[f2] = np.where(df[f1].isin(titles), [df[f2],df[f1]], [df[f1],df[f2]])\n",
    "    df[f2], df[f3] = np.where(df[f2].isin(titles), [df[f3],df[f2]], [df[f2],df[f3]])\n",
    "    # Split middlename\n",
    "    df[add_subscript(letter, ['middle1', 'middle2'])] = df['middlename_'+letter].str.split(' ', n=1, expand=True)\n",
    "    # Split surname\n",
    "    df['sur1_'+letter], df['sur2_'+letter] = df['surname_'+letter], None\n",
    "    #df[add_subscript(letter, ['sur1', 'sur2'])] = df['surname_'+letter].str.split(' ', n=1, expand=True)\n",
    "    # Firstname3 no longer needed as it will only contain titles\n",
    "    return df.drop(f3, axis=1)\n",
    "\n",
    "\n",
    "def standardise_postcode(df, pc):\n",
    "    # Remove all white space from the postcodes and make them uppercase\n",
    "    df[pc] = df[pc].str.replace(' ','').str.upper()\n",
    "    # Replace missing postcodes with 'None'\n",
    "    df[pc] = np.where((df[pc] == 'NAN') | (df[pc] == '') | (df[pc].isnull()), None, df[pc])\n",
    "    # Convert the type to string\n",
    "    return df.astype({pc:str})\n",
    "\n",
    "\n",
    "def standardise_sex(df, sex):\n",
    "    # Convert the sex to uppercase\n",
    "    df[sex] = df[sex].str.upper()\n",
    "    # Change 'M' and 'MALE' to 1 and 'F' and 'FEMALE' to 2\n",
    "    df.loc[(df[sex] == 'M') | (df[sex] == 'MALE'), sex] = 1\n",
    "    df.loc[(df[sex] == 'F') | (df[sex] == 'FEMALE'), sex] = 2\n",
    "    # Convert the type to a float\n",
    "    return df.astype({sex:float})\n",
    "\n",
    "\n",
    "def standardise_dob(df, dob):\n",
    "    df[dob] = pd.to_datetime(df[dob], dayfirst=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_derived_variables(df, letter):\n",
    "    # Make new columns containing only the first three letters and initials of each\n",
    "    # part of each name. Also create a column for initials\n",
    "    df = short_names(df, letter)\n",
    "    # Create day, month and year variables for each dataset\n",
    "    dob = 'dob_' + letter\n",
    "    df['daybirth_'+letter] = df[dob].dt.day\n",
    "    df['monthbirth_'+letter] = df[dob].dt.month\n",
    "    df['yearbirth_'+letter] = df[dob].dt.year\n",
    "    # Separate out the different parts of the postcode\n",
    "    return split_postcode(df, letter)\n",
    "   \n",
    "    \n",
    "def short_names(df, letter):\n",
    "    # Create columns containing the first three letters and initial of each name\n",
    "    f1, f2, m1, m2, s1, s2 = add_subscript(letter, ['first1', 'first2', 'middle1', 'middle2', 'sur1', 'sur2'])\n",
    "    for name in [f1, f2, m1, m2, s1, s2]:\n",
    "        df['short'+name] = df[name].str[:3]\n",
    "        df['init'+name] = df[name].str[0]\n",
    "    # Combine all of the initial columns to give a person's initials\n",
    "    df['initials_'+letter] = df[['init'+f1,'init'+f2,'init'+m1,'init'+m2,'init'+s1,'init'+s2]].apply(\n",
    "            lambda row: row.str.cat(sep=''), axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "def split_postcode(df, letter):\n",
    "    # Split out the postcode area and district\n",
    "    pc, area, district = add_subscript(letter, ['postcode', 'pcarea', 'pcdistrict'])\n",
    "    df[area] = df[pc].str.extract('\\A([A-Z]{1,2})', expand=True)\n",
    "    df[district] = df[pc].str.extract('\\A([A-Z]{1,2}[0-9]{1,2})[0-9]', expand=True)\n",
    "    df[district] = np.where((df[district].isnull()) & (df[pc].str.match('[A-Z]{1,2}[0-9]{1,2}')),\n",
    "                              df[pc], df[district])\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_subscript(letter, args):\n",
    "    # Add the letter as a subscript to each variable\n",
    "    args_subscript = []\n",
    "    for arg in args:\n",
    "        args_subscript.append(str(arg) + '_' + letter)\n",
    "    return args_subscript\n",
    "\n",
    "\n",
    "def remove_subscript(args):\n",
    "    # Remove the subscript from each variable\n",
    "    args_no_subscript = []\n",
    "    for arg in args:\n",
    "        args_no_subscript.append(str(arg).split('_')[0])\n",
    "    return args_no_subscript\n",
    "\n",
    "\n",
    "def exact_matching(dfA, dfB):\n",
    "    # Exact matching is just rule-based matching on all of the columns\n",
    "    return rule_based_matching(dfA, dfB, remove_subscript(dfA.columns.values), False)\n",
    "\n",
    "\n",
    "def create_matchkeys():\n",
    "    # Read in the matchkeys file\n",
    "    file = open('./raw_data/working_matchkeys.txt')\n",
    "    lines = file.read().splitlines()\n",
    "    file.close()\n",
    "    matchkeys = []\n",
    "    keep_multiple = False\n",
    "    for line in lines:\n",
    "        if re.match('Include multiple matches', line):\n",
    "            # Set the value of keep_multiple to True or False as appropriate. If neither\n",
    "            # True or False was inputted, raise a value error.\n",
    "            value = re.search('Include multiple matches *=(.*)', line).group(1).replace(' ','')\n",
    "            if value.upper() == 'TRUE':\n",
    "                keep_multiple = True\n",
    "            elif value.replace(' ','').upper() == 'FALSE':\n",
    "                keep_multiple = False\n",
    "            else:\n",
    "                raise ValueError('\"Include multiple matches\" in the file matchkeys.txt '+\\\n",
    "                                 'must be either \"true\" or \"false\" but received \"' + value + '\"')\n",
    "        elif line == '':\n",
    "            # Do nothing as it's a blank line\n",
    "            continue\n",
    "        else:\n",
    "            # Add the matchkey to the list of matchkeys\n",
    "            matchkeys.append(line.replace(' ','').split(','))\n",
    "    return keep_multiple, matchkeys\n",
    "\n",
    "\n",
    "def rule_based_matching(dfA, dfB, matchkey, keep_multiple):\n",
    "    # Create the variable names for dfA and dfB\n",
    "    left = add_subscript('a', matchkey)\n",
    "    right = add_subscript('b', matchkey)\n",
    "    # Join on the columns given by args\n",
    "    linked = dfA.merge(dfB, left_on = left, right_on = right, how = 'inner')\n",
    "    multiple_matches = None\n",
    "    if keep_multiple == False:\n",
    "        # Remove the multiple matches\n",
    "        linked['multi_match'] = linked['id_a'].map(linked['id_a'].value_counts()>1)\\\n",
    "                                | linked['id_b'].map(linked['id_b'].value_counts()>1)\n",
    "        multiple_matches = len(linked[linked['multi_match']])\n",
    "        linked = linked[linked['multi_match'] == False].drop('multi_match', axis=1)\n",
    "    # Add a column 'Match_Status' that has value 1 if the match is correct and 0 otherwise\n",
    "    linked['Match_Status'] = np.where(linked['id_a'] == linked['id_b'],1,0)\n",
    "    true_positives = len(linked[linked['Match_Status']==1])\n",
    "    false_positives = len(linked[linked['Match_Status']==0])\n",
    "    # Find the residuals\n",
    "    residuals = dfA.merge(dfB, left_on = left, right_on = right, how = 'outer')\n",
    "    # Calculate residuals from dfA. We want to include all records from dfA that\n",
    "    # are not in linked but we don't want to include any records from dfB (as\n",
    "    # these would just be a row of null values)\n",
    "    residualsA = residuals[residuals['id_a'].notnull() &\n",
    "                           ~residuals['id_a'].isin(linked['id_a'])][dfA.columns.values]\\\n",
    "                           .drop_duplicates()\n",
    "    # Calculate residuals from dfB. We want to include all records from dfB that\n",
    "    # are not in linked but we don't want to include any records from dfA (as\n",
    "    # these would just be a row of null values)\n",
    "    residualsB = residuals[residuals['id_b'].notnull() & \n",
    "                           ~residuals['id_b'].isin(linked['id_b'])][dfB.columns.values]\\\n",
    "                           .drop_duplicates()\n",
    "    unmatched = len(residualsB)\n",
    "    # Print information on the number of true positives, false positives and \n",
    "    # unmatched records for this matchkey\n",
    "    print_match_rate(False, left == dfA.columns.values.tolist(), matchkey,\n",
    "                     multiple_matches, true_positives, false_positives, unmatched, None)\n",
    "    # Update dfA and dfB to remove the matched records\n",
    "    dfA = residualsA\n",
    "    dfB = residualsB\n",
    "    return (dfA, dfB, (true_positives, false_positives))\n",
    "\n",
    "\n",
    "def print_match_rate(final, exact, matchkey, multiple_matches, true_positives,\n",
    "                     false_positives, unmatched, false_negatives):\n",
    "    # Print details of the type of matching and the number of true positives, false\n",
    "    # positives etc. in a readable form\n",
    "    if final:\n",
    "        print('Overall match rate')\n",
    "    elif exact:\n",
    "        print('Exact matching')\n",
    "    else:\n",
    "        print('Matchkey = ' + ', '.join(matchkey))\n",
    "        if multiple_matches != None:\n",
    "            print('Multiple matches = ' + str(multiple_matches))\n",
    "    print('True positives = ' + str(true_positives))\n",
    "    print('False positives = ' + str(false_positives))\n",
    "    if unmatched != None:\n",
    "        print('Unmatched = ' + str(unmatched))\n",
    "    if false_negatives != None:\n",
    "        print('False negatives = ' + str(false_negatives))\n",
    "    print('')\n",
    "\n",
    "\n",
    "def update_match_rate(true_positives, false_positives, link_status):\n",
    "    # Update the number of true positives and false positives found\n",
    "    true_positives += link_status[0]\n",
    "    false_positives += link_status[1]\n",
    "    return(true_positives, false_positives)\n",
    "\n",
    "\n",
    "def get_precision(true_positives, false_positives):\n",
    "    # Calculate the precision as a percentage to 2 decimal places\n",
    "    return round(true_positives/(true_positives+false_positives)*100, 2)\n",
    "\n",
    "\n",
    "def get_recall(true_positives, false_negatives):\n",
    "    # Calculate the recall as a percentage to 2 decimal places\n",
    "    return round(true_positives/(true_positives+false_negatives)*100, 2)\n",
    "\n",
    "\n",
    "# Read in the data, clean it and create any derived variables so it is ready for matching\n",
    "dfA, dfB = read_data()\n",
    "dfA = clean_data(dfA, \"a\")\n",
    "dfB = clean_data(dfB, \"b\")\n",
    "dfA = create_derived_variables(dfA, \"a\")\n",
    "dfB = create_derived_variables(dfB, \"b\")\n",
    "   \n",
    "# Set the number of false negatives to be the length of dataset B and the number \n",
    "# of true positives and false positives to be zero as we haven't made any matches\n",
    "false_neg = len(dfB)\n",
    "true_pos, false_pos = 0, 0\n",
    "    \n",
    "# Run the exact matching and update the number of true positives and false positives\n",
    "dfA, dfB, link_status = exact_matching(dfA, dfB)\n",
    "true_pos, false_pos = update_match_rate(true_pos, false_pos, link_status)\n",
    "    \n",
    "# Read in the list of matchkeys and convert each to a list of variables on which to match\n",
    "keep_multiple, matchkeys = create_matchkeys()\n",
    "    \n",
    "# For each matchkey, run the rule-based matching and then update the number of \n",
    "# true positives and false positives\n",
    "for matchkey in matchkeys:\n",
    "    dfA, dfB, link_status = rule_based_matching(dfA, dfB, matchkey, keep_multiple)\n",
    "    true_pos, false_pos = update_match_rate(true_pos, false_pos, link_status)\n",
    "    \n",
    "# Calculate the number of false negatives\n",
    "false_neg -= true_pos\n",
    "    \n",
    "# Print overall summary information and calculate the precision and recall\n",
    "print_match_rate(True, False, None, None, true_pos, false_pos, None, false_neg)\n",
    "print('Precision = ' + str(get_precision(true_pos, false_pos)) + '%')\n",
    "print('Recall = ' + str(get_recall(true_pos, false_neg)) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d3d156-1333-4780-9b5a-37cfae0ec522",
   "metadata": {
    "tags": []
   },
   "source": [
    "## M and U Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fc6de8cb-bbbd-4070-b44d-ee299e5a24a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id_a', 'firstname_a', 'middlename_a', 'surname_a', 'sex_a',\n",
       "       'postcode_a', 'dob_a', 'first1_a', 'first2_a', 'title_a', 'middle1_a',\n",
       "       'middle2_a', 'sur1_a', 'sur2_a', 'shortfirst1_a', 'initfirst1_a',\n",
       "       'shortfirst2_a', 'initfirst2_a', 'shortmiddle1_a', 'initmiddle1_a',\n",
       "       'shortmiddle2_a', 'initmiddle2_a', 'shortsur1_a', 'initsur1_a',\n",
       "       'shortsur2_a', 'initsur2_a', 'initials_a', 'daybirth_a', 'monthbirth_a',\n",
       "       'yearbirth_a', 'pcarea_a', 'pcdistrict_a'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfA.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd799524-345f-48dc-8829-db2aeb1b150c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id_b</th>\n",
       "      <th>firstname_b</th>\n",
       "      <th>middlename_b</th>\n",
       "      <th>surname_b</th>\n",
       "      <th>sex_b</th>\n",
       "      <th>postcode_b</th>\n",
       "      <th>dob_b</th>\n",
       "      <th>first1_b</th>\n",
       "      <th>first2_b</th>\n",
       "      <th>title_b</th>\n",
       "      <th>...</th>\n",
       "      <th>shortsur1_b</th>\n",
       "      <th>initsur1_b</th>\n",
       "      <th>shortsur2_b</th>\n",
       "      <th>initsur2_b</th>\n",
       "      <th>initials_b</th>\n",
       "      <th>daybirth_b</th>\n",
       "      <th>monthbirth_b</th>\n",
       "      <th>yearbirth_b</th>\n",
       "      <th>pcarea_b</th>\n",
       "      <th>pcdistrict_b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9315</th>\n",
       "      <td>HG42PU_15</td>\n",
       "      <td>MUHAMMAD</td>\n",
       "      <td>None</td>\n",
       "      <td>HARRISON</td>\n",
       "      <td>1.0</td>\n",
       "      <td>HG4QP</td>\n",
       "      <td>1992-03-04</td>\n",
       "      <td>MUHAMMAD</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>HAR</td>\n",
       "      <td>H</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MH</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>HG</td>\n",
       "      <td>HG4QP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9316</th>\n",
       "      <td>BD111HA_105</td>\n",
       "      <td>ESMERELDA LENA</td>\n",
       "      <td>None</td>\n",
       "      <td>ROBINSON</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BD111HA</td>\n",
       "      <td>1988-01-07</td>\n",
       "      <td>ESMERELDA</td>\n",
       "      <td>LENA</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>ROB</td>\n",
       "      <td>R</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>ELR</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>BD</td>\n",
       "      <td>BD11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9317</th>\n",
       "      <td>BD111HA_113</td>\n",
       "      <td>None</td>\n",
       "      <td>CALLUM</td>\n",
       "      <td>JAMES</td>\n",
       "      <td>1.0</td>\n",
       "      <td>BD111HA</td>\n",
       "      <td>1993-02-03</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>JAM</td>\n",
       "      <td>J</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CJ</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>BD</td>\n",
       "      <td>BD11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9318</th>\n",
       "      <td>BD111HA_12</td>\n",
       "      <td>DAISY</td>\n",
       "      <td>FLORENCE</td>\n",
       "      <td>CHAPMAN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BD111HA</td>\n",
       "      <td>1984-03-02</td>\n",
       "      <td>DAISY</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>CHA</td>\n",
       "      <td>C</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>DFC</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>BD</td>\n",
       "      <td>BD11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9319</th>\n",
       "      <td>BD111HA_125</td>\n",
       "      <td>DAISY LYLA</td>\n",
       "      <td>None</td>\n",
       "      <td>CHAPMAN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>BD111HA</td>\n",
       "      <td>1993-02-05</td>\n",
       "      <td>DAISY</td>\n",
       "      <td>LYLA</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>CHA</td>\n",
       "      <td>C</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>DLC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1993.0</td>\n",
       "      <td>BD</td>\n",
       "      <td>BD11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11111</th>\n",
       "      <td>YO607ES_62</td>\n",
       "      <td>LYAL</td>\n",
       "      <td>LYLA</td>\n",
       "      <td>COLLINS</td>\n",
       "      <td>2.0</td>\n",
       "      <td>YO607ES</td>\n",
       "      <td>1988-01-01</td>\n",
       "      <td>LYAL</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>COL</td>\n",
       "      <td>C</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>LLC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1988.0</td>\n",
       "      <td>YO</td>\n",
       "      <td>YO60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11112</th>\n",
       "      <td>YO607ES_74</td>\n",
       "      <td>FLORRIE</td>\n",
       "      <td>THEA</td>\n",
       "      <td>BOOTH</td>\n",
       "      <td>2.0</td>\n",
       "      <td>YO607ES</td>\n",
       "      <td>1990-02-07</td>\n",
       "      <td>FLORRIE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>BOO</td>\n",
       "      <td>B</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>FTB</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1990.0</td>\n",
       "      <td>YO</td>\n",
       "      <td>YO60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11113</th>\n",
       "      <td>YO607ES_77</td>\n",
       "      <td>None</td>\n",
       "      <td>MUHAMMAD</td>\n",
       "      <td>COLLINS</td>\n",
       "      <td>1.0</td>\n",
       "      <td>YO607ES</td>\n",
       "      <td>1987-01-01</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>COL</td>\n",
       "      <td>C</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>MC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>YO</td>\n",
       "      <td>YO60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11114</th>\n",
       "      <td>YO607ES_79</td>\n",
       "      <td>FLORENCE</td>\n",
       "      <td>None</td>\n",
       "      <td>ALLEN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>YO607ES</td>\n",
       "      <td>1984-02-02</td>\n",
       "      <td>FLORENCE</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>ALL</td>\n",
       "      <td>A</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>FA</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>YO</td>\n",
       "      <td>YO60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11115</th>\n",
       "      <td>YO607ES_84</td>\n",
       "      <td>COLLINS</td>\n",
       "      <td>GEORGE</td>\n",
       "      <td>MUHAMMAD</td>\n",
       "      <td>1.0</td>\n",
       "      <td>YO607ES</td>\n",
       "      <td>1987-01-02</td>\n",
       "      <td>COLLINS</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>...</td>\n",
       "      <td>MUH</td>\n",
       "      <td>M</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>CGM</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>YO</td>\n",
       "      <td>YO60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1801 rows Ã— 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id_b     firstname_b middlename_b surname_b  sex_b postcode_b  \\\n",
       "9315     HG42PU_15        MUHAMMAD         None  HARRISON    1.0      HG4QP   \n",
       "9316   BD111HA_105  ESMERELDA LENA         None  ROBINSON    2.0    BD111HA   \n",
       "9317   BD111HA_113            None       CALLUM     JAMES    1.0    BD111HA   \n",
       "9318    BD111HA_12           DAISY     FLORENCE   CHAPMAN    2.0    BD111HA   \n",
       "9319   BD111HA_125      DAISY LYLA         None   CHAPMAN    2.0    BD111HA   \n",
       "...            ...             ...          ...       ...    ...        ...   \n",
       "11111   YO607ES_62            LYAL         LYLA   COLLINS    2.0    YO607ES   \n",
       "11112   YO607ES_74         FLORRIE         THEA     BOOTH    2.0    YO607ES   \n",
       "11113   YO607ES_77            None     MUHAMMAD   COLLINS    1.0    YO607ES   \n",
       "11114   YO607ES_79        FLORENCE         None     ALLEN    2.0    YO607ES   \n",
       "11115   YO607ES_84         COLLINS       GEORGE  MUHAMMAD    1.0    YO607ES   \n",
       "\n",
       "           dob_b   first1_b first2_b title_b  ... shortsur1_b initsur1_b  \\\n",
       "9315  1992-03-04   MUHAMMAD     None    None  ...         HAR          H   \n",
       "9316  1988-01-07  ESMERELDA     LENA    None  ...         ROB          R   \n",
       "9317  1993-02-03       None     None    None  ...         JAM          J   \n",
       "9318  1984-03-02      DAISY     None    None  ...         CHA          C   \n",
       "9319  1993-02-05      DAISY     LYLA    None  ...         CHA          C   \n",
       "...          ...        ...      ...     ...  ...         ...        ...   \n",
       "11111 1988-01-01       LYAL     None    None  ...         COL          C   \n",
       "11112 1990-02-07    FLORRIE     None    None  ...         BOO          B   \n",
       "11113 1987-01-01       None     None    None  ...         COL          C   \n",
       "11114 1984-02-02   FLORENCE     None    None  ...         ALL          A   \n",
       "11115 1987-01-02    COLLINS     None    None  ...         MUH          M   \n",
       "\n",
       "      shortsur2_b initsur2_b initials_b daybirth_b monthbirth_b yearbirth_b  \\\n",
       "9315         None       None         MH        4.0          3.0      1992.0   \n",
       "9316         None       None        ELR        7.0          1.0      1988.0   \n",
       "9317         None       None         CJ        3.0          2.0      1993.0   \n",
       "9318         None       None        DFC        2.0          3.0      1984.0   \n",
       "9319         None       None        DLC        5.0          2.0      1993.0   \n",
       "...           ...        ...        ...        ...          ...         ...   \n",
       "11111        None       None        LLC        1.0          1.0      1988.0   \n",
       "11112        None       None        FTB        7.0          2.0      1990.0   \n",
       "11113        None       None         MC        1.0          1.0      1987.0   \n",
       "11114        None       None         FA        2.0          2.0      1984.0   \n",
       "11115        None       None        CGM        2.0          1.0      1987.0   \n",
       "\n",
       "      pcarea_b pcdistrict_b  \n",
       "9315        HG        HG4QP  \n",
       "9316        BD         BD11  \n",
       "9317        BD         BD11  \n",
       "9318        BD         BD11  \n",
       "9319        BD         BD11  \n",
       "...        ...          ...  \n",
       "11111       YO         YO60  \n",
       "11112       YO         YO60  \n",
       "11113       YO         YO60  \n",
       "11114       YO         YO60  \n",
       "11115       YO         YO60  \n",
       "\n",
       "[1801 rows x 32 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88711e9a-a334-4d1c-b575-068a23a1a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------- #\n",
    "# -- Create PES & CEN gold standard --- #\n",
    "# ------------------------------------- #\n",
    "\n",
    "# Read in mock census and PES data\n",
    "CEN = pd.read_csv('./raw_data/Mock_Rwanda_Data_Census.csv')\n",
    "PES = pd.read_csv('./raw_data/Mock_Rwanda_Data_Pes.csv')\n",
    "\n",
    "# join on unique ID\n",
    "gold_standard = CEN.merge(PES, left_on = 'id_indi_cen', right_on = 'id_indi_pes', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b481113-3f65-4bcb-8e5e-9d0cbc5ff3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "firstnm\n",
      "lastnm\n",
      "sex\n",
      "month\n",
      "year\n",
      "  variable   m_value\n",
      "0  firstnm  0.889849\n",
      "1   lastnm  0.914081\n",
      "2      sex  0.907500\n",
      "3    month  0.531401\n",
      "4     year  0.680851\n"
     ]
    }
   ],
   "source": [
    "# --------------------------- #\n",
    "# --------- M VALUES -------- #\n",
    "# --------------------------- #\n",
    "\n",
    "# Create empty dataframe to add m values to\n",
    "m_values = pd.DataFrame([])\n",
    "\n",
    "# Probabilistic linkage variables\n",
    "MU_variables = ['firstnm', 'lastnm', 'sex', 'month', 'year']\n",
    "\n",
    "# Store total number of records for use in calculation\n",
    "total_records = len(gold_standard)\n",
    "    \n",
    "# --- for loop --- #\n",
    "\n",
    "# For each variable:\n",
    "for v in MU_variables:\n",
    "    print(v)  \n",
    "    \n",
    "    # Remove missing rows\n",
    "    gold_standard.dropna(subset=[v + '_cen'], inplace=True)\n",
    "    gold_standard.dropna(subset=[v + '_pes'], inplace=True)\n",
    "    \n",
    "    # counting total number of non-missing probabilistic variables\n",
    "    total_records = len(gold_standard)\n",
    "    \n",
    "    # Create a column that stores whether or not there is exact agreement for that pair      \n",
    "    gold_standard[v + \"_exact\"] = np.where(gold_standard[v + '_pes'] == gold_standard[v + '_cen'], 1, 0)\n",
    "\n",
    "    # Use the sum_col function to create a total number of pairs with exact agreement\n",
    "    exact = gold_standard[v + \"_exact\"].sum()\n",
    "      \n",
    "    # Divide the total number of exact matches by the total number of records\n",
    "    value = exact / total_records\n",
    "\n",
    "    # Store the results in a data frame\n",
    "    m_values = m_values.append(pd.DataFrame({'variable': v, 'm_value': value}, index=[1]), ignore_index=True)\n",
    "\n",
    "print(m_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd29b855-3f91-4a9c-8420-7eb36b09d75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  variable   u_value\n",
      "0  firstnm  0.008909\n",
      "1   lastnm  0.004444\n",
      "2      sex  0.623932\n",
      "3    month  0.081712\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------ #\n",
    "# ---------- U VALUES ---------- #\n",
    "# ------------------------------ #\n",
    "\n",
    "# ----- Sample for calculating U values from full census ----- #\n",
    "\n",
    "# DataFrame to append to\n",
    "u_values = pd.DataFrame([])\n",
    "\n",
    "# condition to reset loop if u value is 0\n",
    "restart = True\n",
    "while restart:\n",
    "    \n",
    "    # For name variables:\n",
    "    for v in MU_variables:\n",
    "        \n",
    "        # Randomly sort datasets\n",
    "        CEN = CEN.sample(frac = 1).reset_index(drop=True)\n",
    "        PES = PES.sample(frac = 1).reset_index(drop=True)\n",
    "\n",
    "        # Add a ID column to join on\n",
    "        sample = pd.merge(CEN, PES, left_index = True, right_index = True)\n",
    "\n",
    "        # Remove missing rows\n",
    "        sample.dropna(subset=[v + '_cen'], inplace=True)\n",
    "        sample.dropna(subset=[v + '_pes'], inplace=True)\n",
    "\n",
    "        # Count\n",
    "        total = len(sample)\n",
    "\n",
    "        # Agreement count\n",
    "        sample[v + \"_exact\"] = np.where(sample[v + '_pes'] == sample[v + '_cen'], 1, 0)\n",
    "\n",
    "        # Create a total number of pairs with exact agreement\n",
    "        exact = sample[v + \"_exact\"].sum()\n",
    "\n",
    "        # Proportion\n",
    "        value = exact / total\n",
    "        \n",
    "        # condition to reset loop if u value is 0\n",
    "        if value > 0 and v != 'year':\n",
    "                      \n",
    "            # Append to DataFrame\n",
    "            u_values = u_values.append(pd.DataFrame({'variable': v, 'u_value': value}, index=[1]), ignore_index=True)\n",
    "\n",
    "            # Add DOB U value if needed\n",
    "            # u_values = u_values.append(pd.DataFrame(data = ({'u_value': [(1/(365*80)) * 100], 'variable': ['dob']})), ignore_index = True)\n",
    "            \n",
    "        else:\n",
    "            restart = False\n",
    "            break\n",
    "            \n",
    "# Print\n",
    "print(u_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3fc10553-3df9-4a0d-8961-f253125d7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------- #\n",
    "# --------------- SAVE ---------------- #\n",
    "# ------------------------------------- #\n",
    "\n",
    "# Spark DataFrame\n",
    "m_values.to_csv('./processed_data/m_values.csv', header = True, index = False)\n",
    "u_values.to_csv('./processed_data/u_values.csv', header = True, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bd7813-4aca-44fb-969e-f20eb879a829",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Expectation Maximisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e0759b6-4525-4831-be04-40416cf8d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/miniconda3/lib/python3.9/site-packages (3.6.5)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/miniconda3/lib/python3.9/site-packages (from nltk) (2021.11.10)\n",
      "Requirement already satisfied: joblib in /opt/miniconda3/lib/python3.9/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: click in /opt/miniconda3/lib/python3.9/site-packages (from nltk) (8.0.1)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.9/site-packages (from nltk) (4.61.2)\n"
     ]
    }
   ],
   "source": [
    "# Required Libraries\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2e269a24-ea91-427e-8e58-9ef36667f253",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration = 1\n",
      "0.026630395405244065\n",
      "[0.81481851 0.98979709 0.99265778]\n",
      "[0.05728426 0.07500899 0.11662356]\n",
      "iteration = 2\n",
      "0.005976142821984644\n",
      "[0.7427302  0.99851483 0.99909998]\n",
      "[0.057724   0.05623927 0.09903317]\n",
      "iteration = 3\n",
      "0.0006490029101322416\n",
      "[0.71905732 0.99963915 0.99979782]\n",
      "[0.05793757 0.0494629  0.09263928]\n",
      "iteration = 4\n",
      "2.8346104615366645e-05\n",
      "[0.71415814 0.99989512 0.99994232]\n",
      "[0.05796686 0.04796008 0.09122563]\n",
      "iteration = 5\n",
      "9.334648589668653e-07\n",
      "[0.7132759  0.99996832 0.99998264]\n",
      "[0.0579706  0.04767858 0.09096319]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This script is intended to give a Python demonstration of applying the Expectation-Maximisation algorithm to generate m and u parameters\n",
    "used in probabilistic data linkage models.\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.metrics import edit_distance\n",
    "\n",
    "# Mock rows of data\n",
    "data = {'ID_1': [1,1,1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "        'Forename_1': ['Charlie','Charlie','Charlie','John','John','Charlie','Charlie','John','Dave','Dave','Dave','Steve','Steve','Charles'],\n",
    "        'Surname_1': ['Smith','Smith','Smith','Taylor','Taylor','Bob','James','Taylor','Wright','Wright','Wright','Johnson','Johnson','Johnson'],\n",
    "        'value_1': [50,200,125,10,15,15,15,30,100,500,0,20,45,200],\n",
    "        'Country_1': ['United Kingdom','United Kingdom','United Kingdom','Germany','Germany','Germany','Germany','Germany','Spain','Spain',\n",
    "                      'Spain','Brazil','Brazil','Germany'],\n",
    "        'DOB_1': ['02/10/1995','02/10/1995','02/10/1995','15/12/2000','15/12/2000','15/12/2000','15/12/2000','15/12/2000','01/01/1970',\n",
    "                  '01/01/1970','01/01/1970','25/08/2002','25/08/2002','25/08/2002']\n",
    "       }\n",
    "        \n",
    "# Create df1 using the above data\n",
    "df1 = pd.DataFrame(data)\n",
    "\n",
    "# Mock rows of data\n",
    "data = {'ID_2': [1,1,1,1,1,1,1,1,1,1,1,1],\n",
    "        'Forename_2': ['Charlie','Charles','Charlie','John','Jon','John','Dave','David','Dave','Steve','Steve','Steve'],\n",
    "        'Surname_2': ['Johnson','Smith','Smith','Taylor','Taylor','Taylor','Wright','Wright','Wright','Johnson','Johnson','Johnson'],\n",
    "        'value_2': [50,200,125,10,15,30,100,500,0,20,45,200],\n",
    "        'Country_2': ['Spain','United Kingdom','United Kingdom','Germany','Germany','Germany','Spain','Spain','Spain','Brazil',\n",
    "                      'Brazil','Brazil'],\n",
    "        'DOB_2': ['02/10/1995','02/10/1995','02/10/1995','15/12/2000','15/12/2000','15/12/2000','01/01/1970','01/01/1970','01/01/1970',\n",
    "                  '25/08/2002','25/08/2002','25/08/2002']\n",
    "       }\n",
    "\n",
    "# Create df2 using the above rows (also specify column headers)\n",
    "df2 = pd.DataFrame(data)\n",
    "\n",
    "# ------------------------------------------- #\n",
    "# ----------------- Blocking ---------------- #\n",
    "# ------------------------------------------- #\n",
    "\n",
    "# Here we just form all possible candidate pairs as df1 and df2 are small\n",
    "combined_blocks = pd.merge(left=df1,\n",
    "                          right=df2,\n",
    "                          how=\"inner\",\n",
    "                          left_on =['ID_1'],\n",
    "                          right_on=['ID_2']\n",
    "                          )\n",
    "\n",
    "# ------------------------------------------------------------------- #\n",
    "# ----------------- Calculate agreement vector pairs ---------------- #\n",
    "# ------------------------------------------------------------------- #\n",
    "\n",
    "'''Agreement vector is created which is then inputted into the EM Algorithm.\n",
    "Set v1, v2, v3, v4, v5... as the agreement variables\n",
    "\n",
    "Choose here what variables require partial agreement (edit distance - e.g. forename) and ...\n",
    "... what variables do not (e.g. DOB, Postcode)'''\n",
    "\n",
    "# Select agreement variables (e.g. forename, surname, DOB, Postcode, Country of Birth)\n",
    "v1 = 'Forename'\n",
    "v2 = 'Surname'\n",
    "v3 = 'Country'\n",
    "\n",
    "# All agreement variables used to calculate match weights & probabilities\n",
    "all_variables = [v1, v2, v3]\n",
    "\n",
    "# Variables using partial agreement (string similarity)\n",
    "edit_distance_variables = [v1, v2]\n",
    "\n",
    "# Cut off values for edit distance variables\n",
    "# Example: Set agreement for pairs with forename edit distances below 0.35 to 0.00\n",
    "# If no cutoff is required then remove variable from 'edit_distance_variables' and add to 'remaining_variables'\n",
    "cutoff_values = [0.35, 0.35]\n",
    "\n",
    "# Remaining Variables - Only zero or full agreement for these\n",
    "remaining_variables = [v3]\n",
    "\n",
    "# Agreement Vector Columns for each variable - used for EM\n",
    "for variable in all_variables:\n",
    "    combined_blocks[variable + '_agree'] = np.where(combined_blocks[variable + '_1'] == combined_blocks[variable + '_2'], 1., 0.)\n",
    "\n",
    "# Save to New Dataframe - Agreement Matrix\n",
    "agreement_matrix = combined_blocks[[v1 + '_agree', \n",
    "                                    v2 + '_agree',\n",
    "                                    v3 + '_agree']]\n",
    "\n",
    "# ------------------------------------------------------------------- #\n",
    "# ------------- B Expectation Maximization Algorithm  --------------- #\n",
    "# ------------------------------------------------------------------- #\n",
    "\n",
    "'''\n",
    "EM - Algorithm Code    \n",
    "Input: Agreement vector matrix + initial M/U values + initial prior\n",
    "Output: - M/U values for each variable + prior\n",
    "'''\n",
    "\n",
    "# Arrays used for EM for loops\n",
    "j_record_pairs = np.arange(0, agreement_matrix.values.shape[0])\n",
    "i_variable_parameters = np.arange(0, agreement_matrix.values.shape[1])\n",
    "\n",
    "# Number of Record Pairs\n",
    "number_record_pairs = agreement_matrix.values.shape[0]\n",
    "\n",
    "# Number of Variables \n",
    "number_variable_parameters = agreement_matrix.values.shape[1]\n",
    "\n",
    "# Initial M and U values for each variable\n",
    "m_parameter = np.full(number_variable_parameters, 0.9)\n",
    "u_parameter = np.full(number_variable_parameters, 0.1)\n",
    "\n",
    "# Inital prior: Proportion of all candidate record pairs that we think are true\n",
    "# This initial prior does not need to be accurate\n",
    "prior_initial = 0.10\n",
    "\n",
    "# Value required for convergence\n",
    "deltamu_convergence = 0.00001\n",
    "\n",
    "# Max no. of iterations allowed\n",
    "max_iteration = 100\n",
    "\n",
    "# ---------- Initialise Starting Variables -------------------------- #\n",
    "# Document used alongside code: Data Linkage and Record Linkage Techniques, Chapter 9 (Herzog 2007)\n",
    "#   Initialise Placeholder Variables for:\n",
    "#   Gamma Matrix                -> gamma\n",
    "#   Indicator Function          -> gj (For M Values)\n",
    "#   Indicator Function          -> gj (For U Values)\n",
    "#   Prior Estimate              -> p_hat\n",
    "#   Initial (ith) m Estimate    -> m_i\n",
    "#   Initial (ith) u Estimate    -> u_i\n",
    "# ------------------------------------------------------------------- #\n",
    "\n",
    "# Array Copy of agreement matrix dataframe\n",
    "gamma = np.copy(agreement_matrix)\n",
    "\n",
    "# Array of zeros (will get updated)\n",
    "gj_m = np.zeros(agreement_matrix.values.shape[0])\n",
    "\n",
    "# Array of zeros (will get updated)\n",
    "gj_u = np.zeros(agreement_matrix.values.shape[0])\n",
    "\n",
    "# Array Copy of inital prior value (will get updated)\n",
    "prior = np.copy(prior_initial)\n",
    "\n",
    "# Array Copy of intial m values (will get updated)\n",
    "m_1 = np.copy(m_parameter)\n",
    "\n",
    "# Array Copy of intial u values (will get updated)\n",
    "u_1 = np.copy(u_parameter)\n",
    "\n",
    "# Delta MU intial value - we want this to converge\n",
    "delta_mu = 5.\n",
    "\n",
    "# Start at iteration 0 \n",
    "iteration_count = 0\n",
    "\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "# ----------------- Start Main Loop - Run Until Stopping Criteria Met ----------------- #\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "\n",
    "while (delta_mu > deltamu_convergence) & (iteration_count < max_iteration):\n",
    "    \n",
    "    # ------ Run E Step ------ #\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate the Indicator Variable gj_m and gj_u for all j record pairs (Eq 9.5 & 9.5*)\n",
    "    \"\"\"\n",
    "\n",
    "    # Iterate over all record pairs\n",
    "    for j in j_record_pairs:\n",
    "        \n",
    "        # Initialise Product for gm_u and gj_u\n",
    "        p_g_agree_vector_match = 1.\n",
    "        p_g_agree_vector_nonmatch = 1.\n",
    "\n",
    "        # Iterate over all Variables\n",
    "        for i in i_variable_parameters:\n",
    "        \n",
    "            p_g_agree_vector_match = (p_g_agree_vector_match * (m_1[i] ** gamma[j, i]) * (1 - m_1[i]) ** (1 - gamma[j, i]))\n",
    "            p_g_agree_vector_nonmatch = (p_g_agree_vector_nonmatch * (u_1[i] ** gamma[j, i]) * (1 - u_1[i]) ** (1 - gamma[j, i]))\n",
    "\n",
    "        # Calculate indicator functions gj_m and gj_u (9.5 and 9.5*)\n",
    "        gj_m[j] = prior * p_g_agree_vector_match / (prior * p_g_agree_vector_match + (1 - prior) * p_g_agree_vector_nonmatch)\n",
    "        gj_u[j] = (1 - prior) * p_g_agree_vector_nonmatch / (prior * p_g_agree_vector_match + (1 - prior) * p_g_agree_vector_nonmatch)\n",
    "\n",
    "        # Error Check on indicator functions gj_m and gj_u (Check if Division by Zero)\n",
    "        if gj_m[j] == 0 or gj_u[j] == 0:\n",
    "            error = \"gj_m[j] == 0 or gj_u[j] == 0\"\n",
    "            end = True\n",
    "            break\n",
    "        else:\n",
    "            end = False\n",
    "\n",
    "    if end:\n",
    "        print(error)\n",
    "        break\n",
    "\n",
    "    # ------ End of E Step ------ #\n",
    "\n",
    "    # ------ Run M Step --------- #\n",
    "    \n",
    "    \"\"\"\n",
    "    Calculate Estimates for m, u and p, for all record pairs (Eq 9.6, 9,7, 9.8)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Record Original m_i and u_i\n",
    "    previous_m = np.copy(m_1)\n",
    "    previous_u = np.copy(u_1)\n",
    "\n",
    "    # Initialise Denominator Sum (Eqn 9.6)\n",
    "    # Start gj_m and gj_u sum at zero\n",
    "    gj_m_sum = 0.\n",
    "    gj_u_sum = 0.\n",
    "\n",
    "    # Initialise Estimate for new m1 and u1, start sum at zero\n",
    "    m_1 = np.zeros(agreement_matrix.values.shape[1])\n",
    "    u_1 = np.zeros(agreement_matrix.values.shape[1])\n",
    "\n",
    "    # Iterate over all j records\n",
    "    for j in j_record_pairs:\n",
    "        \n",
    "        # Iterate over all i parameter variables\n",
    "        for i in i_variable_parameters:\n",
    "            \n",
    "            # Update the m and u values for each variable\n",
    "            m_1[i] = m_1[i] + gamma[j, i] * gj_m[j]\n",
    "            u_1[i] = u_1[i] + gamma[j, i] * gj_u[j]\n",
    "\n",
    "        # Update gj_m and gj_u sum:\n",
    "        gj_m_sum = gj_m_sum + gj_m[j]\n",
    "        gj_u_sum = gj_u_sum + gj_u[j]\n",
    "\n",
    "    # Error check on gj_m and gj_u Sum, if either equals 0 then break\n",
    "    if gj_m_sum == 0 or gj_u_sum == 0:\n",
    "        error = \"Error: gj_m_sum == 0 or  gj_u_sum == 0\"\n",
    "        print(error)\n",
    "        break\n",
    "\n",
    "    # Calculate new m estimates -> Eqn 9.6 in full\n",
    "    m_1 = m_1 / gj_m_sum\n",
    "\n",
    "    # Calculate new u estimates -> Eqn 9.7 in full\n",
    "    u_1 = u_1 / gj_u_sum\n",
    "\n",
    "    # Calculate new prior estimate -> Eqn 9.8 in full\n",
    "    prior = gj_m_sum / len(j_record_pairs)\n",
    "    \n",
    "    # Also calculate prior odds\n",
    "    prior_odds = (prior)/(1 - prior)\n",
    "    \n",
    "    # Error Check on current M & U Values\n",
    "    for i in i_variable_parameters:\n",
    "        \n",
    "        # Check if mu values are equal to 0 or 1, if so break\n",
    "        if m_1[i] == 0 or m_1[i] == 1 or u_1[i] == 0 or u_1[i] == 1:\n",
    "            error = \"Error: m_1[i] == 0 or m_1[i] == 1 or u_1[i] == 0 or u_1[i] == 1\"\n",
    "            end = True\n",
    "            break\n",
    "        else:\n",
    "            end = False\n",
    "    if end:\n",
    "        print(error)\n",
    "        break\n",
    "\n",
    "    # ------ End of M Step ----------------- #\n",
    "    # ------ Check Iteration Criteria ------ #\n",
    "\n",
    "    # Calculate Delta(m) + Delta(u)\n",
    "    # This is what we want to converge\n",
    "    delta_mu = np.dot( (m_1 - previous_m), (m_1 - previous_m)) + np.dot((u_1 - previous_u), (u_1 - previous_u))\n",
    "    \n",
    "    # Update Iteration Count and begin next iteration (until convergence)\n",
    "    iteration_count = iteration_count + 1\n",
    "    print('iteration = ' + str(iteration_count))\n",
    "    print(delta_mu)\n",
    "    print(m_1)\n",
    "    print(u_1)\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# ------------- C Output Information (M & U)------------------------ #\n",
    "# ------------------------------------------------------------------ #\n",
    "\n",
    "# Create M and U Dataframes\n",
    "m_values = pd.DataFrame([m_1], columns=[\"{}_m\".format(name) for name in all_variables])\n",
    "u_values = pd.DataFrame([u_1], columns=[\"{}_u\".format(name) for name in all_variables])\n",
    "\n",
    "# Concatenate m and u dataframes into single dataframe\n",
    "mu_values = pd.concat([m_values, u_values], axis=1)\n",
    "\n",
    "# Add mu_values onto \"combined_blocks\"\n",
    "# This results in every pair/row having m and u values attached \n",
    "combined_blocks_mu = pd.concat([combined_blocks, mu_values], axis=1, ignore_index=False).ffill()\n",
    "\n",
    "''' This is the end of the m and u parameter estimation. Next, we will apply this to score our candidate records'''\n",
    "\n",
    "# --------------------------------------------------------------- #\n",
    "# ------------------ D Calculate Match Scores  ------------------ #\n",
    "# --------------------------------------------------------------- #\n",
    "\n",
    "''' An agreement value between 0 and 1 is calculated for each agreeement variable '''  \n",
    "''' This is done for every candidate record pair '''  \n",
    "    \n",
    "# --------------------------------------------------------------- #\n",
    "# ------------- Variables using String Similarity  -------------- #\n",
    "# --------------------------------------------------------------- #\n",
    "\n",
    "'''Edit Distance Calculator'''\n",
    "\n",
    "for variable in edit_distance_variables:\n",
    "    \n",
    "    # Calculate Agreement Score based using edit distance function\n",
    "    combined_blocks_mu[variable + \"_agreement\"] = combined_blocks_mu.apply(lambda x: 1 - edit_distance(x[variable + \"_1\"], x[variable + \"_2\"]) / max(len(x[variable + \"_1\"]), len(x[variable + \"_2\"])), axis=1)\n",
    "\n",
    "# ----------------------------------------------------------------------- #\n",
    "# ------------------ CUTOFF Points for Partials ------------------------- #\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "'''\n",
    "Cut off Value for variables using edit distance\n",
    "E.g. May want to set any partial scores below 0.35 to 0.00 for variable v1\n",
    "'''\n",
    "\n",
    "#for variable, cutoff in zip(edit_distance_variables, cutoff_values):\n",
    "#\n",
    "#    # If agreement below a certain level, set agreement to 0. Else, leave agreeement as it is\n",
    "#    combined_blocks_mu[variable + \"_agreement\"] = np.where(combined_blocks_mu[variable + \"_agreement\"] <= cutoff, 0., combined_blocks_mu[variable + \"_agreement\"])\n",
    "\n",
    "# ------------------------------------------------------------------- #\n",
    "# ------------- Variables NOT using String Similarity  -------------- #\n",
    "# ------------------------------------------------------------------- #\n",
    "\n",
    "for variable in remaining_variables:\n",
    "\n",
    "    # Calculate 1/0 Agreement Score (no partial scoring)\n",
    "    combined_blocks_mu[variable + \"_agreement\"] = np.where(combined_blocks_mu[variable + \"_1\"] == combined_blocks_mu[variable + \"_2\"], 1., 0.)\n",
    "\n",
    "# ----------------------------------------------------------------------- #\n",
    "# ------------------------- FINAL WEIGHTS ------------------------------- #\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "'''\n",
    "Calculate weights for all matching variables\n",
    "Our Partial Weight formula covers full agreement and disagreement\n",
    "'''\n",
    "\n",
    "for variable in all_variables:\n",
    "      \n",
    "  # Weight Calculation - Covers full agreement, full disagreement and partial agreement cases   \n",
    "  combined_blocks_mu[variable + '_weight'] = ((combined_blocks_mu[variable + \"_m\"] / combined_blocks_mu[variable + \"_u\"]) -\n",
    "                                                    (((combined_blocks_mu[variable + \"_m\"]/combined_blocks_mu[variable + \"_u\"]) - \n",
    "                                                      ((1-combined_blocks_mu[variable + \"_m\"])/(1-combined_blocks_mu[variable + \"_u\"])))*(1-combined_blocks_mu[variable + \"_agreement\"])))\n",
    "                                                                                   \n",
    "\n",
    "# Columns to sum\n",
    "columns = [v1 + \"_weight\", \n",
    "           v2 + \"_weight\", \n",
    "           v3 + \"_weight\"]\n",
    "\n",
    "# Sum column wise across the above columns - create match score (sum of weights for each variable)\n",
    "combined_blocks_mu[\"match_score\"] = combined_blocks_mu[columns].sum(axis=1)\n",
    "\n",
    "# Posterior Odds ratio\n",
    "combined_blocks_mu['posterior_odds_ratio'] = combined_blocks_mu[\"match_score\"] * prior_odds\n",
    "\n",
    "# Posterio Probability\n",
    "combined_blocks_mu['posterior_probability'] = combined_blocks_mu[\"posterior_odds_ratio\"] / (1 + combined_blocks_mu[\"posterior_odds_ratio\"])\n",
    "\n",
    "# ----------------------------------------------------------------------- #\n",
    "# ------------------------  Finalising Output --------------------------- #\n",
    "# ----------------------------------------------------------------------- #\n",
    "\n",
    "# Drop individual weight columns  \n",
    "for variable in all_variables:  \n",
    "    column = [variable + \"_weight\"]      \n",
    "    combined_blocks_mu = combined_blocks_mu.drop(labels=column, axis=1)\n",
    "\n",
    "# Columns to Keep for Final Dataset - add extras if required\n",
    "columns = [\"Forename_1\", \"Surname_1\", \"Country_1\",\n",
    "          'Forename_2', 'Surname_2', 'Country_2',\n",
    "          'match_score','posterior_probability']\n",
    "       \n",
    "# Select required Columns before exporting dataset\n",
    "combined_blocks_final = combined_blocks_mu[columns]\n",
    "\n",
    "# Sort\n",
    "combined_blocks_final = combined_blocks_final.sort_values(by = 'posterior_probability', ascending = False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
