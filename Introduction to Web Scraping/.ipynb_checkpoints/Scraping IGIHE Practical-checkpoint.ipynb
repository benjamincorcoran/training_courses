{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371a162c-811f-4281-8194-f242534d4396",
   "metadata": {},
   "source": [
    "# Practical scraping demo\n",
    "\n",
    "Scraping IGHIE.com to get news articles for categories, likes and dislikes authors and full article text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43ec3bb9-6587-42da-a2d4-e89c65982189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the BASE_URL to be the root of the site igihe.com\n",
    "BASE_URL = 'http://www.igihe.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6274842-1239-41e6-b6e8-47a80255219e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function scrape_article that we can use to scrape information \n",
    "# from individual article pages. \n",
    "\n",
    "def scrape_article(article_url):\n",
    "    '''\n",
    "    Scrape an article from igihe.com and get the author, date, full text and thumbs up and down\n",
    "    \n",
    "    Args:\n",
    "        article_url (str): The full URL to the article on igihe.com\n",
    "        \n",
    "    Returns:\n",
    "        list: A list containing strings for the author, date, thumbs up, thumbs down and \n",
    "              full text of the article\n",
    "    '''\n",
    "    \n",
    "    # Get the raw HTML string of the article from the URL\n",
    "    raw_html = requests.get(article_url)\n",
    "    \n",
    "    # Convert the raw HTML string into a BeautifulSoup object \n",
    "    soup = BeautifulSoup(raw_html.text)\n",
    "\n",
    "    # Extract the author and the date of the article using the 'overview-article' class\n",
    "    # this returns both author and date seperated with a \\n character. Use split to seperate\n",
    "    # them into two different variables.\n",
    "    author, date = soup.find(class_='overview-article').get_text().strip().split('\\n')\n",
    "    \n",
    "    # Extract the thumbs up and thumbs down using the classes th-ok and th-no respectively\n",
    "    thumbs_up = soup.find(class_='th-ok').get_text().strip()\n",
    "    thumbs_down = soup.find(class_='th-no').get_text().strip()\n",
    "    \n",
    "    # Extract the full text of the article using the class fulltext\n",
    "    full_text = soup.find(class_='fulltext').get_text()\n",
    "    \n",
    "    # Return a list of author, date, thumbs up, thumbs down and the full article text.\n",
    "    return [author, date, thumbs_up, thumbs_down, full_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38289f2-5080-4c01-b41b-c6377028eb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import pandas as pd \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Get the raw HTML string using requests\n",
    "raw_html = requests.get(BASE_URL)\n",
    "# Convert the raw HTML string into a Beautiful Soup object\n",
    "soup = BeautifulSoup(raw_html.text)\n",
    "\n",
    "# Use the Beautiful soup object to find all of the articles\n",
    "# as all articles have the class 'article-wrap'\n",
    "articles_html = soup.find_all(class_='article-wrap')\n",
    "\n",
    "# Set up a empty list to store our scraped data\n",
    "rows = []\n",
    "\n",
    "# Print out the total number of articles found on the BASE_URL page. \n",
    "print('Number of articles found: ' + str(len(articles_html)))\n",
    "\n",
    "# Loop over every article \n",
    "for article_html in articles_html:\n",
    "    \n",
    "    # The article category is stored in a <h6> tag within the 'article-wrap' class\n",
    "    # scrape the text using get_text and remove excess whitespace using strip\n",
    "    category = article_html.find('h6').get_text().strip()\n",
    "    \n",
    "    # Look for the class 'homenews-title' in the article html \n",
    "    headline_html = article_html.find(class_='homenews-title')\n",
    "    \n",
    "    # If there isn't any 'homenews-title' class then the article is formatted \n",
    "    # differently, but it will have a 'homenews-title2' class. This article format\n",
    "    # does not contain information about photos, videos and audios we need to set\n",
    "    # that manually. \n",
    "    if headline_html is None:\n",
    "        # Look for the homenews-title2 class and scrape the text, stripping out \n",
    "        # any excess whitespace\n",
    "        headline_html = article_html.find(class_='homenews-title2')\n",
    "        headline = headline_html.get_text().strip()\n",
    "        \n",
    "        # Get the article URL from the headline, as the headline is the link \n",
    "        # to the article, this is stored in the href attribute of an a tag\n",
    "        article_url = headline_html.find('a')['href']\n",
    "        \n",
    "        # Use a try / except statement to handle errors when attempting to scrape \n",
    "        # individual article pages, if there is any error, create a blank list to \n",
    "        # add to our scraped data. \n",
    "        try:\n",
    "            # Use the function scrape_article to pull out the article information \n",
    "            # that we are interested in.\n",
    "            article_info = scrape_article('http://www.igihe.com/' + article_url)\n",
    "        except:\n",
    "            article_info = ['','','','','']\n",
    "\n",
    "        # Set the numbers of photos, videos and audios as 0 as these headlines \n",
    "        # do not show the number of photos, videos or audios.\n",
    "        photos = '0'\n",
    "        videos = '0'\n",
    "        audios = '0'\n",
    "\n",
    "    # If there is a 'homenews-title' class in the article, then it is a normally\n",
    "    # formatted article\n",
    "    else:\n",
    "        # Scrape the headline, which is contained in the 'homenews-title' class\n",
    "        # which has already been selected and stored in the variable headline_html\n",
    "        headline = headline_html.get_text().strip()\n",
    "        \n",
    "        # Get the article URL from the headline, as the headline is the link \n",
    "        # to the article, this is stored in the href attribute of an a tag\n",
    "        article_url = headline_html.find('a')['href']\n",
    "\n",
    "        \n",
    "        # Use a try / except statement to handle errors when attempting to scrape \n",
    "        # individual article pages, if there is any error, create a blank list to \n",
    "        # add to our scraped data. \n",
    "        try:\n",
    "            # Use the function scrape_article to pull out the article information \n",
    "            # that we are interested in.\n",
    "            article_info = scrape_article('http://www.igihe.com/' + article_url)\n",
    "        except:\n",
    "            article_info = ['','','','','']\n",
    "        \n",
    "        # Photos are stored in the 'article_photos' class, scrape the text and \n",
    "        # remove excess whitespace\n",
    "        photos = article_html.find(class_='article_photos').get_text().strip()\n",
    "\n",
    "        # If the text inside 'article_photos' is blank then manually set the value \n",
    "        # of photos to 0. \n",
    "        if photos == '':\n",
    "            photos = '0'\n",
    "\n",
    "        # Repeat the same process for photos for 'article_videos' and 'article_audios'\n",
    "        videos = article_html.find(class_='article_videos').get_text().strip()\n",
    "        audios = article_html.find(class_='article_audios').get_text().strip()\n",
    "    \n",
    "    # Add all the scraped information into a list, and add that list to a rows object\n",
    "    rows.append([category, headline, photos, videos, audios, article_url] + article_info)\n",
    "    \n",
    "    \n",
    "# Convert our list of lists into a pandas dataframe and add on the column names\n",
    "df = pd.DataFrame(rows, columns=['category','headline', 'photos', 'videos', 'audios', 'article_url', \n",
    "                                 'author', 'date', 'thumbs_up', 'thumbs_down', 'full_text']) \n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d520dd-492a-48ed-a00c-3c215bae4db5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
